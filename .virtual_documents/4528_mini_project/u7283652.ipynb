import cv2 
import numpy as np 
import matplotlib 
import matplotlib.pyplot as plt 
from scipy.ndimage import gaussian_filter
import os 
import torch 
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
from sklearn.model_selection import train_test_split
import random 
import time
import copy
from tqdm import tqdm
import math


IMAGE_PATH = './jcsmr.jpg'

PATCH_SIZE = 64
STRIDE = 32
AUGMENT = True
AUGMENTATION_MULTIPLIER = 3 
VALIDATION_SPLIT = 0.2 
BATCH_SIZE = 32
NUM_WORKERS = os.cpu_count() // 2

# Canny edge parameters 
CANNY_THRESHOLD_1 = 100 # Lower threshold for hysteresis procedure 
CANNY_THRESHOLD_2 = 200 # Upper threshold for hysteresis procedure 


def display_images(images, titles, cmap='gray', figsize=(15, 5), save_path="canny_edge_plot.png"):
    """Helper func to display multiple images"""
    n_images = len(images)
    plt.figure(figsize=figsize)
    for i in range(n_images):
        plt.subplot(1, n_images, i + 1)
        im = plt.imshow(images[i], cmap=cmap if images[i].ndim == 2 else None)
        plt.title(titles[i])
        plt.axis('off')

    plt.tight_layout()

    if save_path:
        plt.savefig(save_path, dpi=150)
    plt.show()
    plt.close()


print(f"Loading image from: {IMAGE_PATH}")
if not os.path.exists(IMAGE_PATH):
    raise FileNotFoundError(f"file not found at specified path")

# load in bgr format 
img_bgr = cv2.imread(IMAGE_PATH)

if img_bgr is None:
    raise IOError(f"error: could not read image file")

print(f"image shape: {img_bgr.shape}")


# convert to grayscale 
img_gray = cv2.cvtColor(img_bgr, cv2.COLOR_BGR2GRAY) # canny 

# check for shape and type 
print(f"img shape: {img_gray.shape}, dtype: {img_gray.dtype}")


# generate canny edge ground truth
# in: uint8 grayscale img -> out: uint8 binary img (0 or 255)
canny_edges = cv2.Canny(img_gray, threshold1=CANNY_THRESHOLD_1, threshold2=CANNY_THRESHOLD_2)

# convert output (0, 255) to flaot32 map with vals [0, 1] for binary cross entropy loss 
canny = (canny_edges / 255.0).astype(np.float32)

print(f"canny shape: {canny.shape}, dtype: {canny.dtype}, unique values: {np.unique(canny)}")


# --- Visualization --- 
print("Displaying results")
display_images(
    [cv2.cvtColor(img_bgr, cv2.COLOR_BGR2RGB), img_gray, canny], 
    ['Original RGB', "grayscale input", f'Canny (threshold={CANNY_THRESHOLD_1}/{CANNY_THRESHOLD_2})'],
    cmap='gray',
    figsize=(20, 5)
)


def compute_harris_response(Ix: np.ndarray, Iy: np.ndarray, k: float=0.05,
    sigma: float=1.0) ->np.ndarray:
    """Computes the Harris corner response map using the gradient images Ix and Iy.
    You may use the imported gaussian_filter function from scipy.

    Parameters:
        Ix (numpy.ndarray (H, W) [float]): The gradient image in the x-direction.
        Iy (numpy.ndarray (H, W) [float]): The gradient image in the y-direction.
        k (float): Harris detector free parameter, typically between 0.01 and 0.1. Defaults to 0.05.
        sigma (float): Standard deviation for Gaussian filter. Defaults to 1.

    Returns:
        numpy.ndarray (H, W) [float]: The Harris response map.
    """
    # product of derivatives at each pixel
    Ix2 = Ix * Ix
    Iy2 = Iy * Iy 
    Ixy = Ix * Iy 

    # use gaussian filtering to smooth the products
    Ix2_smooth = gaussian_filter(Ix2, sigma=sigma)
    Iy2_smooth = gaussian_filter(Iy2, sigma=sigma)
    Ixy_smooth = gaussian_filter(Ixy, sigma=sigma)

    # Compute the harris response function 
    # R = det(M) - k * (trace(M))**2
    # M is the structure tensor [ [Ix2, Ixy], [Ixy, Iy2] ]

    # Determinant = Ix2 * Iy2 - Ixy**2 
    det_M = Ix2_smooth * Iy2_smooth - (Ixy_smooth * Ixy_smooth)
    # trace = Ix2 + Iy2 
    trace_M = Ix2_smooth + Iy2_smooth

    # Harris response
    R = det_M - k * (trace_M**2)

    return R


def harris_keypoint_nms(response: np.ndarray, threshold: float=0.01,
    window_size: int=5) ->list:
    """Identifies keypoints based on the Harris response using non-maximum suppression.

    Args:
        response (np.ndarray (H, W) [float]): The Harris response map.
        threshold (float, optional): Threshold for keypoint detection. Defaults to 0.01.
        window_size (int, optional): Side-length (odd) of the square window for non-maximum suppression. Defaults to 5.

    Returns:
        list: A list of identified keypoints as (x, y) integer coordinates.
    """
    # ensure window size is odd:
    assert window_size % 2 == 1, "Window size must be odd"

    # height and width of the response map
    height, width = response.shape

    # get half of window for neighbourhood checking
    half_window = window_size // 2

    keypoints = []

    # iterate through all pixels in the response map (excl. border)
    for y in range(half_window, height - half_window):
        for x in range(half_window, width - half_window):
            # skip if response < threshold
            if response[y, x] < threshold:
                continue 
            window = response[y - half_window:y + half_window + 1,
                            x - half_window:x + half_window + 1]
            # check if center pixel is the maximum in the window 
            if response[y, x] == np.max(window):
                # add as keypoint 
                keypoints.append((x, y))
    return keypoints


def compute_DoG(image: np.ndarray, sigma: float=1.6, num_octaves: int=4,
    num_scales: int=5) ->list[list[np.ndarray]]:
    """Computes the Difference of Gaussians (DoG) pyramid for the given image.

    Args:
        image (np.ndarray (H, W) [uint8]): The input grayscale image with shape (H, W).
        sigma (float, optional): The base standard deviation of the Gaussian filter for the first octave. Defaults
            to 1.6, as suggested in SIFT.
        num_octaves (int, optional): Number of octaves in the pyramid. Each octave represents an image downsampled by a
            factor of 2. Defaults to 4.
        num_scales (int, optional): Number of scales per octave (excluding extra layers for DoG computation). Defaults
            to 5.

    Returns:
        list[list[np.ndarray]]: A DoG pyramid, where each inner list represents an octave and contains `num_scales` DoG
            images. Each DoG image has the same shape as the corresponding Gaussian-blurred image in that octave.
            That is, if images in the first octave have shape (H, W) [float32], then the second octave images have
            shape (H//2, W//2) [float32].

    Notes:
        1. The input image should be converted to floating point and **normalized to the range [0,1]** by dividing by 255.0
            to ensure numerical stability.
        2. The standard deviation for each Gaussian blur is computed as:
            sigma_i = sigma * (k^i), where k = 2^(1/num_scales) and i is the scale index.
        3. Since `num_scales + 1` Gaussian images are generated per octave, we get `num_scales` DoG images.
        4. Each **octave** starts from an image and iteratively applies Gaussian smoothing.
           Once all scales are computed, the image is downsampled (reduced to half resolution)
           and used as the base for the next octave.
        5. Use (0, 0) as the kernel size for cv2.GaussianBlur to automatically compute the kernel size.
    """
    img = image.astype(np.float32) / 255.0
    # scale factor
    k = 2.0 ** (1.0 / num_scales)

    dog_pyramid = []
    # curr base img for the octave 
    curr_base_img = img.copy()

    for octave in range(num_octaves):
        # store gaussian blurred imgs for the octave
        gauss_imgs = []
        # store DoG images for this octave 
        dog_imgs = []

        # generate num_scales + 1 gaussian-blurred images for the octave
        for scale in range(num_scales + 1):
            scale_sigma = sigma * (k**scale)
            blurred = cv2.GaussianBlur(curr_base_img, (0,0), scale_sigma)
            gauss_imgs.append(blurred)

            # compute DoG as difference b/w consecutive Gauss imgs 
            if scale > 0:
                DoG = gauss_imgs[scale] - gauss_imgs[scale - 1]
                dog_imgs.append(DoG)
        
        # add the DoG images for this octave to the pyramid 
        dog_pyramid.append(dog_imgs)

        # prepare base img for next octave by downsampling the last gaussian
        # image of the current octave
        if octave < num_octaves - 1:
            curr_base_img = cv2.resize(gauss_imgs[-1],
                                    (gauss_imgs[-1].shape[1] // 2,
                                    gauss_imgs[-1].shape[0] // 2),
                                    interpolation=cv2.INTER_AREA)
    return dog_pyramid


def match_features(descriptors1: np.ndarray, descriptors2: np.ndarray,
    ratio_thresh: float=0.75) ->list:
    """Matches features between two descriptor sets using the L2 distance (np.linalg.norm).
    A feature in the first set (descriptors1) is matched to the closest feature in the second set (descriptors2),
    in the L2 sense, so long as it passes Lowe's Ratio Test.

    Args:
        descriptors1 (np.ndarray (N, 128) [float32]): Descriptors from image 1.
        descriptors2 (np.ndarray (N, 128) [float32]): Descriptors from image 2.
        ratio_thresh (float): Loweâ€™s Ratio Test threshold.

    Returns:
        matches (list[tuple]): Matched keypoint indices [(idx1_0, idx2_0), ..., (idx1_k, idx2_k)].
    """
    matches = []
    # for each descriptor in the first set, init variables to track the 
    # two closest matches 
    for i, desc1 in enumerate(descriptors1):
        best_dist = float('inf')
        second_best_dist = float('inf')
        best_idx = -1

        # compare with all desc in the second set
        for j, desc2 in enumerate(descriptors2):
            # compute L2 dist b/w descriptors 
            dist = np.linalg.norm(desc1 - desc2)

            if dist < best_dist:
                second_best_dist = best_dist
                best_dist = dist
                best_idx = j 
            elif dist < second_best_dist:
                second_best_dist = dist 
        
        # lowe's ratio test 
        if best_dist < ratio_thresh * second_best_dist:
            matches.append((i, best_idx))
    
    return matches


Ix = cv2.Sobel(img_gray, cv2.CV_32F, 1, 0, ksize=3)
Iy = cv2.Sobel(img_gray, cv2.CV_32F, 0, 1, ksize=3)

plt.figure(figsize=(10, 5))
plt.subplot(1, 2, 1)
plt.imshow(Ix, cmap="gray")
plt.title("Ix")
plt.subplot(1, 2, 2)
plt.imshow(Iy, cmap="gray")
plt.title("Iy")
plt.show()


k = 0.04
sigma = 1.0
harris_response = compute_harris_response(Ix, Iy, k, sigma)
# Normalize the response for better visualization
harris_response_vis = np.log(1 + np.abs(harris_response))
harris_response_vis = (harris_response_vis - harris_response_vis.min()) / (
    harris_response_vis.max() - harris_response_vis.min()
)
plt.imshow(harris_response_vis, cmap="jet")
plt.colorbar()
plt.title("Harris Response Map")


threshold = 0.1 * np.max(harris_response)
keypoints = harris_keypoint_nms(harris_response, threshold, 5)
plt.imshow(cv2.cvtColor(img_bgr, cv2.COLOR_BGR2RGB))
plt.scatter(*zip(*keypoints), c="r", s=25, marker="x")
plt.title(f"Detected {len(keypoints)} Harris Corners")
plt.axis("off")
plt.savefig("harris_corners.png")


assert img_gray.shape == harris_response_vis.shape == canny_float.shape, "Input img and ground truth maps must have same dims."


H, W = img_gray.shape
print(f"Image dimensions: H={H}, W={W}")
print(f"Patch Size: {PATCH_SIZE}, Stride: {STRIDE}")


# extract patches
patch_triplets = [] # List to hold (input_patch, harris_patch, canny_patch)

for y in range(0, H - PATCH_SIZE + 1, STRIDE):
    for x in range(0, W - PATCH_SIZE + 1, STRIDE):
        input_patch = img_gray[y : y + PATCH_SIZE, x : x + PATCH_SIZE]
        harris_patch = harris_response_vis[y : y + PATCH_SIZE, x : x + PATCH_SIZE]
        canny_patch = canny[y : y + PATCH_SIZE, x : x + PATCH_SIZE]

        # discard patches with low variance (like most of the sky)
        if np.std(input_patch) < 5: # ex. threshold of 5
            continue

        patch_triplets.append((input_patch, harris_patch, canny_patch))

print(f"Extracted {len(patch_triplets)} original patches.")


# apply augmentation to patches 
if AUGMENT:
    print(f"Augmenting patches with Multiplier: {AUGMENTATION_MULTIPLIER}")
    augmented_patches = []
    for _ in range(AUGMENTATION_MULTIPLIER):
        for input, harris, canny in patch_triplets:
            # rand augmentation
            aug_type = random.choice(['rotate90', 'rotate180', 'rotate270', 'flip_h', 'flip_v', 'none'])

            input_aug, harris_aug, canny_aug = input.copy(), harris.copy(), canny.copy()

            if aug_type == 'rotate90':
                input_aug = cv2.rotate(input_aug, cv2.ROTATE_90_CLOCKWISE)
                harris_aug = cv2.rotate(harris_aug, cv2.ROTATE_90_CLOCKWISE)
                canny_aug = cv2.rotate(canny_aug, cv2.ROTATE_90_CLOCKWISE)
            elif aug_type == 'rotate180':
                input_aug = cv2.rotate(input_aug, cv2.ROTATE_180)
                harris_aug = cv2.rotate(harris_aug, cv2.ROTATE_180)
                canny_aug = cv2.rotate(canny_aug, cv2.ROTATE_180)
            elif aug_type == 'rotate270':
                input_aug = cv2.rotate(input_aug, cv2.ROTATE_90_COUNTERCLOCKWISE)
                harris_aug = cv2.rotate(harris_aug, cv2.ROTATE_90_COUNTERCLOCKWISE)
                canny_aug = cv2.rotate(canny_aug, cv2.ROTATE_90_COUNTERCLOCKWISE)
            elif aug_type == 'flip_h':
                input_aug = cv2.flip(input_aug, 1) # Horizontal flip
                harris_aug = cv2.flip(harris_aug, 1)
                canny_aug = cv2.flip(canny_aug, 1)
            elif aug_type == 'flip_v':
                input_aug = cv2.flip(input_aug, 0) # Vertical flip
                harris_aug = cv2.flip(harris_aug, 0)
                canny_aug = cv2.flip(canny_aug, 0)

            augmented_patches.append((input_aug, harris_aug, canny_aug))

    total_patches = patch_triplets + augmented_patches
    print(f"Total patches after augmentation: {len(total_patches)}")

else:
    total_patches = patch_triplets
    print("Skipping augmentation.")


# split data into train and val sets 
train_patches, val_patches = train_test_split(
    total_patches,
    test_size=VALIDATION_SPLIT,
    random_state=42,
    shuffle=True
)

print(f"Training patches: {len(train_patches)}")
print(f"Validation patches: {len(val_patches)}")


# Create torch dataset class

class PatchDataset(Dataset):
    def __init__(self, patch_list):
        self.patch_list = patch_list

    def __len__(self):
        return len(self.patch_list)

    def __getitem__(self, idx):
        input_patch_np, harris_patch_np, canny_patch_np = self.patch_list[idx]

        # Convert to float32 and normalize input patch to [0, 1]
        input_patch_tensor = torch.from_numpy(input_patch_np.astype(np.float32) / 255.0)
        harris_patch_tensor = torch.from_numpy(harris_patch_np.astype(np.float32))
        canny_patch_tensor = torch.from_numpy(canny_patch_np.astype(np.float32))

        # Add channel dimension (C, H, W)
        # *Grayscale images have 1 channel
        input_patch_tensor = input_patch_tensor.unsqueeze(0)
        harris_patch_tensor = harris_patch_tensor.unsqueeze(0)
        canny_patch_tensor = canny_patch_tensor.unsqueeze(0)

        return input_patch_tensor, harris_patch_tensor, canny_patch_tensor


# create datasets and dataloaders

train_dataset = PatchDataset(train_patches)
val_dataset = PatchDataset(val_patches)

train_loader = DataLoader(
    train_dataset,
    batch_size=BATCH_SIZE,
    shuffle=True, 
    num_workers=NUM_WORKERS,
    pin_memory=True 
)

val_loader = DataLoader(
    val_dataset,
    batch_size=BATCH_SIZE,
    shuffle=False,
    num_workers=NUM_WORKERS,
    pin_memory=True
)

print("DataLoaders created successfully.")

# visualize a sample batch
try:
    sample_inputs, sample_harris, sample_canny = next(iter(train_loader))
    print(f"Sample batch - Input shape: {sample_inputs.shape}")     # Should be [BATCH_SIZE, 1, PATCH_SIZE, PATCH_SIZE]
    print(f"Sample batch - Harris GT shape: {sample_harris.shape}") # Should be [BATCH_SIZE, 1, PATCH_SIZE, PATCH_SIZE]
    print(f"Sample batch - Canny GT shape: {sample_canny.shape}")   # Should be [BATCH_SIZE, 1, PATCH_SIZE, PATCH_SIZE]

    # Display the first image from the batch
    plt.figure(figsize=(12, 4))
    plt.subplot(1, 3, 1)
    plt.imshow(sample_inputs[0, 0].numpy(), cmap='gray') 
    plt.title('Sample Input Patch')
    plt.axis('off')

    plt.subplot(1, 3, 2)
    plt.imshow(sample_harris[0, 0].numpy(), cmap='viridis')
    plt.title('Sample Harris GT Patch')
    plt.axis('off')

    plt.subplot(1, 3, 3)
    plt.imshow(sample_canny[0, 0].numpy(), cmap='gray')
    plt.title('Sample Canny GT Patch')
    plt.axis('off')

    plt.tight_layout()
    plt.savefig("batch.png")
    plt.show()

except Exception as e:
    print(f"Could not visualize sample batch: {e}")
    print("DataLoader failed.")

print("\nData Preparation Complete.")


# Plan:
# 1. setup a U-net architecture which is good for image-to-image tasks
# 2. Instantiate two separate models for Harris and Canny
# 3. Implement loss functions and optimizers
# 4. Create a training loop as well as a validation process


# U-Net
class DoubleConv(nn.Module):
    """(Conv -> [BN] -> ReLU) * 2"""
    def __init__(self, in_channels, out_channels, mid_channels=None):
        super().__init__()
        if not mid_channels:
            mid_channels = out_channels
        self.double_conv = nn.Sequential(
            nn.Conv2d(in_channels, mid_channels, kernel_size=3, padding=1, bias=False),
            nn.BatchNorm2d(mid_channels),
            nn.ReLU(inplace=True),
            nn.Conv2d(mid_channels, out_channels, kernel_size=3, padding=1, bias=False),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(inplace=True)
        )

    def forward(self, x):
        return self.double_conv(x)



class Down(nn.Module):
    """Downscaling with MaxPool then DoubleConv"""
    def __init__(self, in_channels, out_channels):
        super().__init__()
        self.maxpool_conv = nn.Sequential(
            nn.MaxPool2d(2),
            DoubleConv(in_channels, out_channels)
        )

    def forward(self, x):
        return self.maxpool_conv(x)

class Up(nn.Module):
    """Upscaling then DoubleConv"""
    def __init__(self, in_channels, out_channels, bilinear=True):
        super().__init__()
        # if bilinear, use normal conv to reduce the number of channels
        if bilinear:
            self.up = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)
            self.conv = DoubleConv(in_channels, out_channels, in_channels // 2)
        else:
            self.up = nn.ConvTranspose2d(in_channels, in_channels // 2, kernel_size=2, stride=2)
            self.conv = DoubleConv(in_channels, out_channels)

    def forward(self, x1, x2):
        x1 = self.up(x1)
        # input is C H W
        diffY = x2.size()[2] - x1.size()[2]
        diffX = x2.size()[3] - x1.size()[3]

        x1 = nn.functional.pad(x1, [diffX // 2, diffX - diffX // 2,
                                    diffY // 2, diffY - diffY // 2])
        x = torch.cat([x2, x1], dim=1)
        return self.conv(x)

class OutConv(nn.Module):
    def __init__(self, in_channels, out_channels):
        super(OutConv, self).__init__()
        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size=1)

    def forward(self, x):
        return self.conv(x)

class UNet(nn.Module):
    def __init__(self, n_channels=1, n_classes=1, bilinear=True):
        super(UNet, self).__init__()
        self.n_channels = n_channels
        self.n_classes = n_classes
        self.bilinear = bilinear

        self.inc = DoubleConv(n_channels, 64)
        self.down1 = Down(64, 128)
        self.down2 = Down(128, 256)
        factor = 2 if bilinear else 1
        self.down3 = Down(256, 512 // factor) 
        self.up1 = Up(512, 256 // factor, bilinear) 
        self.up2 = Up(256, 128 // factor, bilinear) 
        self.up3 = Up(128, 64, bilinear)
        self.outc = OutConv(64, n_classes)
        # sigmoid activation for BCELoss or regression between 0 and 1
        self.sigmoid = nn.Sigmoid()

    def forward(self, x):
        x1 = self.inc(x)
        x2 = self.down1(x1)
        x3 = self.down2(x2)
        x4 = self.down3(x3)
        x = self.up1(x4, x3)
        x = self.up2(x, x2)
        x = self.up3(x, x1)
        logits = self.outc(x)
        # Apply sigmoid for final output
        output = self.sigmoid(logits)
        return output


# Training configuration 
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"Using device: {device}")

LEARNING_RATE_HARRIS = 1e-4 # tune later
LEARNING_RATE_CANNY = 1e-4
NUM_EPOCHS = 100

# init models 
model_harris = UNet(n_channels=1, n_classes=1).to(device)
model_canny = UNet(n_channels=1, n_classes=1).to(device)

# loss functions
# MSE loss for harris heatmap regression (b/w 0 & 1)
criterion_harris = nn.MSELoss()
# BCE loss for canny edge prediction (0 or 1, model outputs sigmoid probability)
criterion_canny = nn.BCELoss()

# Optimizers
optimizer_harris = optim.Adam(model_harris.parameters(), lr=LEARNING_RATE_HARRIS)
optimizer_canny = optim.Adam(model_canny.parameters(), lr=LEARNING_RATE_CANNY)


# Training loop
def train_model(model_h, criterion_h, optimizer_h,
                model_c, criterion_c, optimizer_c,
                dataloaders, device, num_epochs=25):
    """
    Trains both Harris and Canny models.
    Args:
        model_h: Harris prediction model
        criterion_h: Harris loss function
        optimizer_h: Harris optimizer
        model_c: Canny prediction model
        criterion_c: Canny loss function
        optimizer_c: Canny optimizer
        dataloaders (dict): Dictionary containing 'train' and 'val' DataLoaders
        device: torch.device ('cuda' or 'cpu')
        num_epochs (int): Number of epochs to train for

    Returns:
        tuple: (best_harris_model_state, best_canny_model_state) containing state_dicts
               of the models with the lowest validation loss for each task.
    """
    start = time.time()

    best_harris_model_wts = copy.deepcopy(model_h.state_dict())
    best_canny_model_wts = copy.deepcopy(model_c.state_dict())
    best_harris_loss = float('inf')
    best_canny_loss = float('inf')

    # store losses for plotting 
    history = {'train_loss_h': [], 'val_loss_h': [], 'train_loss_c': [], 'val_loss_c': []}

    for epoch in range(num_epochs):
        print(f'Epoch {epoch+1}/{num_epochs}')
        print('-' * 10)

        for phase in ['train', 'val']:
            if phase == 'train':
                # set model to train mode
                model_h.train() 
                model_c.train()
            else:
                # set models to eval mode
                model_h.eval() 
                model_c.eval()

            running_loss_h = 0.0
            running_loss_c = 0.0

            # iterate over data
            loader = dataloaders[phase]
            pbar = tqdm(loader, desc=f"{phase.capitalize()} Epoch {epoch+1}")

            for inputs, harris, canny in pbar:
                inputs = inputs.to(device)
                harris = harris.to(device)
                canny = canny.to(device)

                # harris task 
                optimizer_h.zero_grad()
                # forward pass 
                with torch.set_grad_enabled(phase == 'train'):
                    harris_pred = model_h(inputs)
                    loss_h = criterion_h(harris_pred, harris)

                    # backward + optimize
                    if phase == 'train':
                        loss_h.backward()
                        optimizer_h.step()

                # canny task
                optimizer_c.zero_grad()
                # forward pass
                with torch.set_grad_enabled(phase == 'train'):
                    canny_pred = model_c(inputs)
                    loss_c = criterion_c(canny_pred, canny)

                    # backward + optimize
                    if phase == 'train':
                        loss_c.backward()
                        optimizer_c.step()

                # stats
                batch_loss_h = loss_h.item() * inputs.size(0)
                batch_loss_c = loss_c.item() * inputs.size(0)
                running_loss_h += batch_loss_h
                running_loss_c += batch_loss_c

                # update progress bar 
                pbar.set_postfix({'Loss H': batch_loss_h / inputs.size(0), 'Loss C': batch_loss_c / inputs.size(0)})

            epoch_loss_h = running_loss_h / len(loader.dataset)
            epoch_loss_c = running_loss_c / len(loader.dataset)

            print(f'{phase.capitalize()} Harris Loss: {epoch_loss_h:.4f} Canny Loss: {epoch_loss_c:.4f}')

            # store history
            if phase == 'train':
                history['train_loss_h'].append(epoch_loss_h)
                history['train_loss_c'].append(epoch_loss_c)
            else: 
                history['val_loss_h'].append(epoch_loss_h)
                history['val_loss_c'].append(epoch_loss_c)

                if epoch_loss_h < best_harris_loss:
                    best_harris_loss = epoch_loss_h
                    best_harris_model_wts = copy.deepcopy(model_h.state_dict())
                    print(f"  -> New best Harris validation loss: {best_harris_loss:.4f}")
                if epoch_loss_c < best_canny_loss:
                    best_canny_loss = epoch_loss_c
                    best_canny_model_wts = copy.deepcopy(model_c.state_dict())
                    print(f"  -> New best Canny validation loss: {best_canny_loss:.4f}")
        print()

    time_elapsed = time.time() - start
    print(f'Training complete in {time_elapsed // 60:.0f}m {time_elapsed % 60:.0f}s')
    print(f'Best val Harris Loss: {best_harris_loss:4f}')
    print(f'Best val Canny Loss: {best_canny_loss:4f}')

    # Load best model weights back into models
    model_h.load_state_dict(best_harris_model_wts)
    model_c.load_state_dict(best_canny_model_wts)
    return model_h, model_c, history 


# Run training
dataloaders = {'train': train_loader, 'val':val_loader}

# Start
trained_model_h, trained_model_c, history = train_model(
    model_harris, criterion_harris, optimizer_harris,
    model_canny, criterion_canny, optimizer_canny,
    dataloaders, device, num_epochs=NUM_EPOCHS
)

print("Training finished")

# save the model
torch.save(trained_model_h.state_dict(), 'unet_harris_best.pth')
torch.save(trained_model_c.state_dict(), 'unet_canny_best.pth')
print("Best model weights saved.")

# plot training hist
plt.figure(figsize=(12, 5))

plt.subplot(1, 2, 1)
plt.plot(history['train_loss_h'], label='Train Harris Loss')
plt.plot(history['val_loss_h'], label='Val Harris Loss')
plt.title('Harris Model Loss')
plt.xlabel('Epochs')
plt.ylabel('Loss (MSE)')
plt.legend()
plt.grid(True)

plt.subplot(1, 2, 2)
plt.plot(history['train_loss_c'], label='Train Canny Loss')
plt.plot(history['val_loss_c'], label='Val Canny Loss')
plt.title('Canny Model Loss')
plt.xlabel('Epochs')
plt.ylabel('Loss (BCE)')
plt.legend()
plt.grid(True)

plt.tight_layout()
plt.show()
plt.savefig('training_history.png')





trained_model_h.eval()
trained_model_c.eval()

# orig img dims
H, W = img_gray.shape

# calc padding needed to make dimensions % by patch size
pad_h = (PATCH_SIZE - H % PATCH_SIZE) % PATCH_SIZE
pad_w = (PATCH_SIZE - W % PATCH_SIZE) % PATCH_SIZE

img_gray_norm = img_gray.astype(np.float32) / 255.0 
input_padded = cv2.copyMakeBorder(img_gray_norm, 0, pad_h, 0, pad_w, cv2.BORDER_REFLECT)

H_pad, W_pad = input_padded.shape

print(f"Original shape: ({H}, {W})")
print(f"Padding Added : H={pad_h}, W={pad_w}")
print(f"Padded Shape  : ({H_pad}, {W_pad})")


# Create canvas for preds
harris_pred_canvas = np.zeros((H_pad, W_pad), dtype=np.float32)
canny_pred_canvas = np.zeros((H_pad, W_pad), dtype=np.float32)

# patch based predictions
print(f"predicting on full image using patches")
with torch.no_grad():
    for y in tqdm(range(0, H_pad, PATCH_SIZE), desc="Processing Rows"):
        for x in range(0, W_pad, PATCH_SIZE):
            # extract patch
            input_patch_np = input_padded[y : y + PATCH_SIZE, x : x + PATCH_SIZE]
            # preprocess patch
            input_patch_tensor = torch.from_numpy(input_patch_np)
            input_patch_tensor = input_patch_tensor.unsqueeze(0).unsqueeze(0) # adding batch and channel dims -> (1,1,P,P)
            input_patch_tensor = input_patch_tensor.to(device)
            # predict
            harris_pred_patch_tensor = trained_model_h(input_patch_tensor)
            canny_pred_patch_tensor = trained_model_c(input_patch_tensor)
            # post process patch
            harris_pred_patch_np = harris_pred_patch_tensor.squeeze().cpu().numpy() # (P, P)
            canny_pred_patch_np = canny_pred_patch_tensor.squeeze().cpu().numpy() # (P, P)

            # place into canvas 
            harris_pred_canvas[y : y + PATCH_SIZE, x : x + PATCH_SIZE] = harris_pred_patch_np
            canny_pred_canvas[y : y + PATCH_SIZE, x : x + PATCH_SIZE] = canny_pred_patch_np

# copying pred back to orig size
harris_pred_final = harris_pred_canvas[0:H, 0:W]
canny_pred_final = canny_pred_canvas[0:H, 0:W]


# Visualizing the predictions 
plt.figure(figsize=(18, 10))

# row 1: input and ground truths
plt.subplot(2, 3, 1)
plt.imshow(img_gray, cmap='gray')
plt.title('Original Grayscale input')
plt.axis('off')

plt.subplot(2, 3, 2)
vmin_h, vmax_h = np.percentile(harris_normalized, [5, 99.5])
plt.imshow(harris_normalized, cmap='gray', vmin=vmin_h, vmax=vmax_h)
plt.title('Harris Ground Truth')
plt.axis('off')

plt.subplot(2, 3, 3)
canny = (canny_edges / 255.0).astype(np.float32)
plt.imshow(canny, cmap='gray')
plt.title('Canny Ground Truth')
plt.axis('off')

# row 2: predictions
plt.subplot(2, 3, 5)
plt.imshow(harris_pred_final, cmap='gray', vmin=vmin_h, vmax=vmax_h)
plt.title('Predicted Harris (CNN)')
plt.axis('off')

plt.subplot(2, 3, 6)
# plt.imshow(canny_pred_final, cmap='gray', vmin=0, vmax=1)
# plt.title('Predicted Canny (CNN Sigmoid)')
canny_pred_binary = (canny_pred_final > 0.5).astype(np.float32) # Threshold at 0.5
plt.imshow(canny_pred_binary, cmap='gray')
plt.title('Predicted Canny (CNN > 0.5)')
plt.axis('off')

plt.tight_layout()
plt.savefig('final_evaluation.png')
plt.show()


plt.figure(figsize=(8, 6))
plt.imshow(harris_pred_final, cmap='viridis') # Use a colormap
plt.colorbar()
plt.title("CNN Harris Prediction Heatmap (Before NMS)")
plt.axis('off')
plt.savefig("cnn_harris_heatmap.png")
plt.show()

# Also check the range and distribution
print(f"CNN Heatmap Stats: Min={np.min(harris_pred_final):.4f}, Max={np.max(harris_pred_final):.4f}, Mean={np.mean(harris_pred_final):.4f}")





thresholds_to_test = [0.5, 0.7, 0.8, 0.9]
plt.figure(figsize=(15, 4))
for i, thresh in enumerate(thresholds_to_test):
    binary_map = (harris_pred_final > thresh).astype(np.uint8)
    num_pixels_above = np.sum(binary_map)
    plt.subplot(1, len(thresholds_to_test), i + 1)
    plt.imshow(binary_map, cmap='gray')
    plt.title(f'> {thresh:.1f} ({num_pixels_above} pix)')
    plt.axis('off')
plt.suptitle("Binary Maps at Different Thresholds (Before NMS)")
plt.savefig("cnn_heatmap_thresholds.png")
plt.show()


# params for CNN keypoint extraction
threshold_cnn = 0.7
window_size_nms = 5
# apply nms to cnn pred 
keypoints_cnn = harris_keypoint_nms(harris_pred_final, threshold_cnn_relative, window_size_nms)
print(f"Detected {len(keypoints_cnn)} corners from CNN prediction.")

if 'keypoints' in locals():
    fig, axes = plt.subplots(1, 2, figsize=(20, 8))

    # Plot Classical Keypoints
    axes[0].imshow(cv2.cvtColor(img_bgr, cv2.COLOR_BGR2RGB))
    if keypoints:
        axes[0].scatter(*zip(*keypoints), c="red", s=25, marker="x", label=f"Classical ({len(keypoints)})") # Red X
    axes[0].set_title(f"Classical Harris Corners")
    axes[0].axis("off")
    axes[0].legend()

    # Plot CNN Keypoints
    axes[1].imshow(cv2.cvtColor(img_bgr, cv2.COLOR_BGR2RGB))
    if keypoints_cnn:
        axes[1].scatter(*zip(*keypoints_cnn), c="blue", s=35, marker="o", label=f"CNN ({len(keypoints_cnn)})") # Blue circles
    axes[1].set_title(f"CNN Predicted Harris Corners (Threshold={threshold_cnn:.2f})")
    axes[1].axis("off")
    axes[1].legend()

    plt.tight_layout()
    plt.savefig("harris_corners_comparison.png")
    plt.show()
else:
    print("Classical keypoints ('keypoints' variable) not found, skipping comparison plot.")


# Quantitative evaluation
# Harris MSE
mse_harris = np.mean((harris_normalized - harris_pred_final)**2)
print(f"Harris Prediction MSE: {mse_harris:.6f}")

# Canny Metrics (using the thresholded prediction)
canny = (canny_edges / 255.0).astype(np.float32)
canny_pred_binary = (canny_pred_final > 0.5).astype(np.uint8)
canny_gt_binary = canny.astype(np.uint8)
assert canny_pred_binary.shape == canny_gt_binary.shape, \
    f"Shape mismatch! Pred: {canny_pred_binary.shape}, GT: {canny_gt_binary.shape}"

accuracy_canny = np.mean(canny_pred_binary == canny_gt_binary)
print(f"Canny Pixel Accuracy: {accuracy_canny:.4f}")

# Intersection over Union (IoU) for edge pixels (class 1)
intersection = np.sum((canny_pred_binary == 1) & (canny_gt_binary == 1))
union = np.sum((canny_pred_binary == 1) | (canny_gt_binary == 1))
iou_canny = intersection / union if union > 0 else 0
print(f"Canny Edge IoU: {iou_canny:.4f}")

# Precision, Recall, F1-Score for edge pixels
true_positives = intersection
predicted_positives = np.sum(canny_pred_binary == 1)
actual_positives = np.sum(canny_gt_binary == 1)

precision = true_positives / predicted_positives if predicted_positives > 0 else 0
recall = true_positives / actual_positives if actual_positives > 0 else 0
f1_score = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0

print(f"Canny Edge Precision: {precision:.4f}")
print(f"Canny Edge Recall: {recall:.4f}")
print(f"Canny Edge F1-Score: {f1_score:.4f}")



